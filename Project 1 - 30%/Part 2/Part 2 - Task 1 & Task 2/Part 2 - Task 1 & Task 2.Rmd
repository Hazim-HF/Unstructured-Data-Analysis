---
title: "Part 2"
output: html_document
---

# Task 1: Topic Modelling Analysis using LDA

```{r}
library(tm)
library(ggplot2)
library(tidytext)
library(topicmodels)
library(dplyr)
library(tidyr)
library(proxy)
library(dbscan)
library(textstem)
```

```{r}
# Read files
docs = VCorpus(DirSource('Tech'))

# Text cleaning
toSpace = content_transformer(function(x, pattern){
  return(gsub(pattern, " ", x))
  })

toSpaceUnicode = content_transformer(function(x) {
  return(gsub("[\u2012\u2013\u2014\u2015-]", " ", x))
         })

custom_stop = c(stopwords("english"), "said", "will", "company")

docs_clean = docs %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(toSpaceUnicode) %>%
  tm_map(removeNumbers) %>%
  tm_map(toSpace, "\\.") %>%
  tm_map(removePunctuation) %>%
  tm_map(removeWords, custom_stop) %>%
  tm_map(stripWhitespace)

# lemmatization
text_data = sapply(docs_clean, as.character)
lemmatize_text = lemmatize_strings(text_data)
docs.lem = Corpus(VectorSource(lemmatize_text))

# Create dtm
dtm = DocumentTermMatrix(docs_clean)
inspect(dtm)
```

```{r}
# apply LDA
k = 3
lda_model = LDA(dtm, k = k, control = list(seed = 1234))
lda_model
```

```{r}
topics = tidy(lda_model, matrix = 'beta')
```

```{r}
# per topic per word probabilities
top_terms = topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_terms
```

```{r}
# top 10 most common term for each topic
top_terms %>%
  group_by(topic) %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(x = term, y = beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE, width = 0.7) +
  facet_wrap(~topic, scales = "free", ncol = 3) +
  coord_flip() +
  scale_x_reordered() +
  scale_fill_brewer(palette = "Set2") +
  labs(
    title = "Top Terms per Topic",
    x = "Term",
    y = "Beta (P(term | topic))"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid = element_blank(),
    strip.text = element_text(face = "bold", size = 14),
    axis.text.y = element_text(size = 12),
    axis.text.x = element_text(size = 12),
    plot.title = element_text(face = "bold", hjust = 0.5, size = 18),
    plot.subtitle = element_text(hjust = 0.5, size = 14)
  )


```

```{r}
topic1vstopic2 %>%
  mutate(
    term = reorder(term, log_ratio),
    direction = ifelse(log_ratio > 0, "Topic 2", "Topic 1")
  ) %>%
  ggplot(aes(term, log_ratio, fill = direction)) +
  geom_col(alpha = 0.85, width = 0.7) +
  geom_hline(yintercept = 0, size = 0.8, color = "gray30") +
  scale_fill_manual(values = c("#E41A1C", "#377EB8")) +  # Colorblind-friendly
  coord_flip() +
  labs(
    title = "Key Term Affinity: Topic 1 vs Topic 2",
    subtitle = "Log2 ratio shows term specialization (Topic2/Topic1)",
    x = NULL,
    y = "Log2 Ratio (Topic2/Topic1)",
    caption = "Positive values = Term favors Topic 2\nNegative values = Term favors Topic 1"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(color = "gray40", margin = margin(b = 15)),
    axis.text.y = element_text(size = 11, color = "black"),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.position = "none",
    plot.caption = element_text(color = "gray30", hjust = 0)
  ) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8))  # Better axis breaks
```

# Task 2: Text Clustering

```{r}
dtm.tfidf = weightTfIdf(dtm)
dtm.tfidf = removeSparseTerms(dtm.tfidf, 0.999)
tfidf.matrix = as.matrix(dtm.tfidf)

# Compute cosine distance
library(proxy)
dist.matrix = dist(tfidf.matrix , method = "cosine")

truth.K=3

library(dbscan)
clustering.kmeans = kmeans(tfidf.matrix , truth.K)
clustering.hierarchical = hclust(dist.matrix , method = "ward.D2")
clustering.dbscan = hdbscan(dist.matrix , minPts = 10)
```

```{r}
plot(clustering.hierarchical,
     main = 'Hierarchical Clustering Dendogram',
     xlab = '')
rect.hclust(clustering.hierarchical, 3)
```

```{r}
master.cluster = clustering.kmeans$cluster
slave.hierarchical = cutree(clustering.hierarchical, k = truth.K)
slave.dbscan = clustering.dbscan$cluster

library(colorspace)
points = cmdscale(dist.matrix, k = 3)
palette = diverge_hcl(truth.K) # Creating a color palette

# layout(matrix(1:3,ncol=1))
plot(points, 
     main = 'K-Means Clustering', 
     col = as.factor(master.cluster),
     pch = 16,
     mai = c(0,0,0,0), 
     mar = c(0,0,0,0),
     xaxt = 'n', 
     yaxt = 'n', 
     xlab = '', 
     ylab ='')

plot(points, 
     main = 'Hierarchical Clustering', 
     col = as.factor(slave.hierarchical),
     pch = 16,
     mai = c(0,0,0,0), 
     mar = c(0,0,0,0),
     xaxt = 'n', 
     yaxt = 'n', 
     xlab = '', 
     ylab ='')

plot(points, 
     main = 'Density-based Clustering', 
     col = as.factor(slave.dbscan),
     pch = 16,
     mai = c(0,0,0,0), 
     mar = c(0,0,0,0),
     xaxt = 'n', 
     yaxt = 'n', 
     xlab = '', 
     ylab ='')
```

```{r}
table(master.cluster)
table(slave.hierarchical)
table(slave.dbscan)
```

```{r}
cost_df = data.frame()

for (i in 1:20) {
  kmeans = kmeans(x=tfidf.matrix, centers=i, iter.max=100)
  cost_df = rbind(cost_df, cbind(i,kmeans$tot.withinss))
}

names(cost_df) = c("cluster","cost")

plot(cost_df$cluster, 
     cost_df$cost,
     pch = 16,
     main = 'Elbow Curve',
     xlab = 'Number of Cluster',
     ylab = 'Total Within Sum of Square')

lines(cost_df$cluster, cost_df$cost)
```
