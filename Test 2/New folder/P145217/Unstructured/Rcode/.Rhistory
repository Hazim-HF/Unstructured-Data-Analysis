length(freq)
ord<-order(freq,decreasing=T)
head(ord)
freq[head(ord)]
dtm<-DocumentTermMatrix(docs,control=list(wordLengths=c(2,20),
bounds=list(global=c(2,30))))
inspect(dtm[1:2,1:100])
freq<-colSums(as.matrix(dtm))
length(freq)
ord<-order(freq,decreasing=T)
head(ord)
freq[head(ord)]
#once we have all above, we can insert to data frame
wf<-data.frame(names(freq),freq)
names(wf)<-c("TERM","FREQ")
head(wf)
findFreqTerms(dtm,lowfreq=10)
findAssocs(dtm,"get",0.3)
library(ggplot2)
Subs<-subset(wf,FREQ>=10)
ggplot(Subs,aes(x=TERM,y=FREQ))+geom_bar(stat="identity")+
theme(axis.text.x=element_text(angle=45,hjust=1))
ggplot(wf,aes(x=TERM,y=FREQ))+geom_bar(stat="identity")+
theme(axis.text.x=element_text(angle=45,hjust=1)) #Show all, include terms that hv small freq
library(wordcloud)
wordcloud(names(freq),freq) #in general
wordcloud(names(freq),freq.min.freq=10) #if we want to focus on the min freq of 10
wordcloud(names(freq),freq,colors=brewer.pal(8,"Darker"))
letterCloud(wf,word="SDA",color="random-light",backgroundColor="black")
###########################
library(textstem)
docs3<-stem_strings(docs)[1]
a<-unlist(str_split(docs3,"[,]"))
docs4<-lemmatize_strings(docs)[1]
b<-unlist(str_split(docs4,"[,]"))
ggplot(Subs,aes=(x=TERM,y=freq))
ggplot(Subs,aes=(x=TERM,y=freq))
ggplot(Subs,aes=(x=TERM,y=FREQ))
ggplot(Subs,aes(x=TERM,y=FREQ))
### Text Analysis I: LDA
library(tidytext)
library(topicmodels)
install.packages('tidytext')
install.packages('topicmodels')
install.packages('tidyr')
install.packages("tidyr")
install.packages("tidyr")
install.packages("dplyr")
### Text Analysis I: LDA
library(tidytext)
library(topicmodels)
library(tidyr)
library(ggplot2)
library(dplyr)
data("AssociatedPress")
ap_lda<-LDA(AssociatedPress,k=2,control=list(seed=1234)) #create two-topic LDA model
ap_topics<-tidy(ap_lda,matrix="beta") #Extract the per-topic-per-word-probabilities
#Find terms that are most common within each topics
ap_top_terms <- ap_topics %>% group_by(topic) %>% top_n(10,beta) %>% ungroup () %>% arrange (topic, -beta)
ap_top_terms%>% mutate(term=reorder(term,beta))%>%
ggplot(aes(term,beta,fill=factor(topic)))+geom_col(show.legend=FALSE)+
facet_wrap(~topic,scales="free")+coord_flip() #visualize the above
beta_spread <- ap_topics %>% mutate (topic=paste0("topic",topic)) %>% spread(topic,beta) %>%
filter (topic1>0.001 | topic2 > 0.001) %>% mutate(log_ratio = log2(topic2/topic1))
beta_spread%>% mutate(term=reorder(term,log_ratio))%>%
ggplot(aes(term,log_ratio))+geom_col(show.legend=FALSE)+coord_flip()
ap_documents<-tidy(ap_lda,matrix="gamma") #Extract the per-document-per-topic-probabilities
ap_documents
tidy(AssociatedPress)%>%filter(document==6)%>%arrange(desc(count)) #Check the most common words in the document, eg document 6
beta_spread <- ap_topics %>% mutate (topic=paste0("topic",topic)) %>% spread(topic,beta) %>%
filter (topic1>0.01 | topic2 > 0.01) %>% mutate(log_ratio = log2(topic2/topic1))
beta_spread%>% mutate(term=reorder(term,log_ratio))%>%
ggplot(aes(term,log_ratio))+geom_col(show.legend=FALSE)+coord_flip()
beta_spread <- ap_topics %>% mutate (topic=paste0("topic",topic)) %>% spread(topic,beta) %>%
filter (topic1>0.0051 | topic2 > 0.0051) %>% mutate(log_ratio = log2(topic2/topic1))
beta_spread%>% mutate(term=reorder(term,log_ratio))%>%
ggplot(aes(term,log_ratio))+geom_col(show.legend=FALSE)+coord_flip()
beta_spread <- ap_topics %>% mutate (topic=paste0("topic",topic)) %>% spread(topic,beta) %>%
filter (topic1>0.002 | topic2 > 0.002) %>% mutate(log_ratio = log2(topic2/topic1))
beta_spread%>% mutate(term=reorder(term,log_ratio))%>%
ggplot(aes(term,log_ratio))+geom_col(show.legend=FALSE)+coord_flip()
beta_spread <- ap_topics %>% mutate (topic=paste0("topic",topic)) %>% spread(topic,beta) %>%
filter (topic1>0.003 | topic2 > 0.003) %>% mutate(log_ratio = log2(topic2/topic1))
beta_spread%>% mutate(term=reorder(term,log_ratio))%>%
ggplot(aes(term,log_ratio))+geom_col(show.legend=FALSE)+coord_flip()
ap_documents
ap_documents$document == 1
ap_documents[ap_documents$document == 1,]
ap_documents[ap_documents$document == 2,]
ap_documents %>% filter(document==2)
# =============================================
# Convert DTM to matrix if needed
dtm_matrix <- as.matrix(associatedpress)
# =============================================
# Convert DTM to matrix if needed
dtm_matrix <- as.matrix(AssociatedPress)
# Initialize vector to store coherence values
coherence_scores <- c()
k_values <- 2:10
# Loop through values of k
for (k in k_values) {
# Fit LDA model
lda_model <- LDA(associatedpress, k = k, method = "Gibbs", control = list(seed = 1234))
# Extract term-topic probabilities (beta matrix)
beta <- posterior(lda_model)$terms
# Compute coherence using textmineR (CaoJuan2009)
coherence <- CalcProbCoherence(phi = beta, dtm = dtm_matrix, M = 10)
mean_coherence <- mean(coherence)
# Store result
coherence_scores <- c(coherence_scores, mean_coherence)
}
# Loop through values of k
for (k in k_values) {
# Fit LDA model
lda_model <- LDA(AssociatedPress, k = k, method = "Gibbs", control = list(seed = 1234))
# Extract term-topic probabilities (beta matrix)
beta <- posterior(lda_model)$terms
# Compute coherence using textmineR (CaoJuan2009)
coherence <- CalcProbCoherence(phi = beta, dtm = dtm_matrix, M = 10)
mean_coherence <- mean(coherence)
# Store result
coherence_scores <- c(coherence_scores, mean_coherence)
}
library(textmineR)
install.packages('textmineR')
library(textmineR)
# Loop through values of k
for (k in k_values) {
# Fit LDA model
lda_model <- LDA(AssociatedPress, k = k, method = "Gibbs", control = list(seed = 1234))
# Extract term-topic probabilities (beta matrix)
beta <- posterior(lda_model)$terms
# Compute coherence using textmineR (CaoJuan2009)
coherence <- CalcProbCoherence(phi = beta, dtm = dtm_matrix, M = 10)
mean_coherence <- mean(coherence)
# Store result
coherence_scores <- c(coherence_scores, mean_coherence)
}
setwd("C:/Users/PC 12/Desktop/P145217/Unstructured/")
setwd("C:/Users/PC 12/Desktop/P145217/Unstructured/")
mytext<-DirSource("TextFile")
# Three common source of data input method
#Example using DirSource
mytext<-DirSource("Attachment 2 - movies-20250505")
#Using tm package
library(tm)
eg3<-c("Hi!","Welcome to STQD6114","Tuesday, 11-1pm")
mytext<-VectorSource(eg3)
# Three common source of data input method
#Example using DirSource
mytext<-DirSource("Attachment 2 - movies-20250505")
# Part 1: Extract Text from Files
setwd("C:/Users/PC 12/Desktop/P145217/Unstructured/Rcode/")
eg1<-read.table("Attachment 1-20250505/GC.txt",fill=T,header=F) #Data GC.txt
eg1[1,]
eg2<-read.csv("Attachment 1-20250505/GC.csv",header=F) #Data GC.csv
eg2[1,]
#Using tm package
library(tm)
eg3<-c("Hi!","Welcome to STQD6114","Tuesday, 11-1pm")
mytext<-VectorSource(eg3)
mycorpus<-VCorpus(mytext)
inspect(mycorpus)
as.character(mycorpus[[1]])
# Three common source of data input method
#Example using DirSource
mytext<-DirSource("Attachment 2 - movies-20250505")
library(tm)
mytext<-DirSource("TextFile")
docs<-VCorpus(mytext)
docs <- tm_map(docs,content_transformer(tolow
er))
docs <- tm_map(docs,content_transformer(tolower))
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, toSpace, ":")
docs <- tm_map(docs, toSpace, "'")
docs <- tm_map(docs, toSpace, ".")
dtm = DocumentTermMatrix(docs, control=list(wordLengths=c(2,30),
bounds=list(global=c(2,30))))
#Present text data numerically, weighted TF-IDF
tdm.tfidf <- weightTfIdf(tdm)
# R submission for test 1
# 4.
monkey = read.csv("C:/Users/PC 12/Downloads/LittleMonkeys.csv",header=F)
monkey
str_view(monkey,"!")
monkey = read.csv("C:/Users/PC 12/Downloads/LittleMonkeys.csv",header=F)
# R submission for test 1
# 4.
library(stringr)
str_view(monkey,"!")
str_view(monkey,"\!")
str_view(monkey,"a")
str_view(monkey,"\!")
str_view(monkey,"\\!")
ss
regexpr(pattern="stat",ss)
#4th function - gregexpr()
gregexpr(pattern="stat",ss)
a = c('aaa','ab')
regexpr(pattern='a', a)
grepexpr(pattern='a', a)
gregexpr(pattern='a', a)
str_view(monkey,"\\!")
# R submission for test 1
# 4.
library(stringr)
monkey = read.csv("C:/Users/PC 12/Downloads/LittleMonkeys.csv",header=F)
monkey
# 4a)
str_view(monkey,"\\!")
str_view(monkey,"a")
#Note: str_view give the output in another browser
str_view(fruit,"an") #view the pattern (for the first time) of dataset
# 4b)
str_view(monkey, "(.)\\1")
# 4c)
str_view(monkey, "[aeiou]{2}")
# 4d)
str_view(monkey, "mon")
# 4d)
gregexpr(pattern="mon",monkey)
# 4d)
gregexpr(pattern="mon",monkey)[1]
# 4d)
gregexpr(pattern="mon",monkey)[[1]]
# 4d)
gregexpr(pattern="mon",monkey)[[1]][1]
# 4d)
gregexpr(pattern="mon",monkey)[[1]][1:]
# 4d)
gregexpr(pattern="mon",monkey)[[1]]
# 4d)
gregexpr(pattern="mon",monkey)[[1]]
# 4d)
gregexpr(pattern="mon",monkey)[[1]][,]
# 4d)
gregexpr(pattern="mon",monkey)[[1]][,1]
# 4d)
gregexpr(pattern="mon",monkey)[[1]][1,1]
# 4d)
gregexpr(pattern="mon",monkey)[[1]][1]
# 4d)
gregexpr(pattern="mon",monkey)[[1]][2]
# 4d)
gregexpr(pattern="mon",monkey)[[1]]
# Index: 55 178 221 344 388 512 554 677 719 840
str_view(monkey, "mon")
# Index: 55 178 221 344 388 512 554 677 719 840
str_view(monkey, "\\bmon")
str_view(monkey, "^mon")
# 4e)
str_count(monkey, "monkeys")
read.table("C:/Users/PC 12/Downloads/BBCnews.txt",fill=T,header=F)
news = read.table("C:/Users/PC 12/Downloads/BBCnews.txt",fill=T,header=F)
head(news)
t = apply(news, MARGIN=1, function(x) {trimws(paste(x, collapse=' '))}) # or use this
t
news = read.table("C:/Users/PC 12/Downloads/BBCnews.txt",fill=T,header=F)
head(news)
news1 = apply(news, MARGIN=1, function(x) {trimws(paste(x, collapse=' '))}) # or use this
head(news1)
mytext<-VectorSource(news1)
mycorpus<-VCorpus(mytext)
inspect(mycorpus)
as.character(mycorpus[[1]])
# 5a)
# Text preprocessing
toSpace = content_transformer(function(x,pattern){return(gsub(pattern," ",x))})
docs = mycorpus
# =====================
# 5.
library(tm)
as.character(mycorpus[[1]])
as.character(mycorpus[[7]])
as.character(mycorpus[[8]])
docs1<-tm_map(docs,toSpace, '-')
as.character(docs1[[8]])
# Remove hyphens '-'
docs1 = tm_map(docs,toSpace, '-')
# Remove numbers
docs2 = tm_map(docs1,removeNumbers)
# Remove punctuation
docs3 = tm_map(docs2,removePunctuation)
# Transform lowercase
docs4 = tm_map(docs3,content_transformer(tolower))
# Remove stopwords
docs4 = tm_map(docs,removeWords,stopwords("english"))
# Strip whitespace
docs5 = tm_map(docs4,tospace,'\\s')
# Strip whitespace
docs5 = tm_map(docs4,toSpace,'\\s')
as.character(docs1[[3]])
as.character(docs1[[2]])
as.character(docs2[[2]])
as.character(docs2[[10]])
as.character(docs2[[11]])
as.character(docs3[[11]])
as.character(docs3[[13]])
as.character(docs4[[13]])
# Remove punctuation
docs3 = tm_map(docs2,removePunctuation)
# Transform lowercase
docs4 = tm_map(docs3,content_transformer(tolower))
# Remove stopwords
docs5 = tm_map(docs4,removeWords,stopwords("english"))
# Strip whitespace
docs6 = tm_map(docs5,toSpace,'\\s')
as.character(docs4[[13]])
as.character(docs3[[13]])
as.character(docs4[[13]])
as.character(docs5[[13]])
as.character(docs6[[13]])
# Strip whitespace
docs6 = tm_map(docs5,toSpace,'\\s+')
as.character(docs6[[13]])
toNothing = content_transformer(function(x,pattern){return(gsub(pattern,"",x))})
docs7 = tm_map(docs6,toNothing,'(^\\s|\\s$)') # Removing leading and trailing whitespae
as.character(docs6[[13]])
as.character(docs7[[13]])
docs7 = tm_map(docs6,toNothing,'(^\\s+|\\s+$)') # Removing leading and trailing whitespae
as.character(docs7[[13]])
inspect(docs7)
inspect(docs7[[1]])
as.character(docs7)
as.character(docs7[[1]])
for i in 1:10
as.character(docs7[[1]])
as.character(docs7[[2]])
as.character(docs7[[3]])
# 5b)
# Lemmatization
getTransformations()
lemmatize_strings(docs7)[1]
# 5b)
# Lemmatization
library(textstem)
lemmatize_strings(docs7)[1]
docs8 = lemmatize_strings(docs7)[1]
as.character(docs8[[1]])
as.character(docs7[[1]])
library(tm)
#docs<-VCorpus(VectorSource(sentences))
docs<-Corpus(VectorSource(sentences))
writeLines(as.character(docs[[30]]))
getTransformations()
#Create custom transformation
toSpace<-content_transformer(function(x,pattern){return(gsub(pattern," ",x))})
as.character(docs[[133]]) #check line 133
docs<-tm_map(docs,toSpace,"-")
as.character(docs[[133]])
docs<-tm_map(docs,removePunctuation)
docs<-tm_map(docs,content_transformer(tolower))
docs<-tm_map(docs,removeNumbers)
docs<-tm_map(docs,removeWords,stopwords("english")) #remove stop words
as.character(docs[[2]])
docs<-tm_map(docs,removeWords,"gp")
docs<-tm_map(docs,stripWhitespace)
library(SnowballC)
docs2<-tm_map(docs,stemDocument) #for stemming the documents
library(textstem)
docs3<-stem_strings(docs)
a<-unlist(str_split(docs3,"[,]"))
docs4<-lemmatize_strings(docs)
b<-unlist(str_split(docs4,"[,]"))
b
docs8<-lemmatize_strings(docs7)
b<-unlist(str_split(docs8,"[,]"))
docs8
lemmatize_strings(docs)
unlist(str_split(lemmatize_strings(docs),"[,]"))
unlist(str_split(lemmatize_strings(docs7),"[,]"))
unlist(str_split(lemmatize_strings(docs),"[,]"))
as.character(docs7[[1]])
lemmatize_strings(docs)
lemmatize_strings(docs7)
docs8<-lemmatize_strings(docs7)
dim(docs8)
type(docs7)
dtype(docs7)
datatype(docs7)
typeof(docs)
typeof(docs7)
mycorpus = Corpus(mytext)
# Convert to corpus
mytext = VectorSource(news1)
mycorpus = Corpus(mytext)
inspect(mycorpus)
as.character(mycorpus[[1]])
# 5a)
# Text preprocessing
# Function to remove matching pattern
toSpace = content_transformer(function(x,pattern){return(gsub(pattern," ",x))})
toNothing = content_transformer(function(x,pattern){return(gsub(pattern,"",x))})
docs = mycorpus
as.character(docs7[[13]])
# Remove hyphens '-'
docs1 = tm_map(docs,toSpace, '-')
# Remove numbers
docs2 = tm_map(docs1,removeNumbers)
# Remove punctuation
docs3 = tm_map(docs2,removePunctuation)
# Transform lowercase
docs4 = tm_map(docs3,content_transformer(tolower))
# Remove stopwords
docs5 = tm_map(docs4,removeWords,stopwords("english"))
# Strip whitespace
docs6 = tm_map(docs5,toSpace,'\\s+') # Remove multiple white space artifacts that come from previous preprocessing
docs7 = tm_map(docs6,toNothing,'(^\\s+|\\s+$)') # Removing leading and trailing whitespae
# 5b)
# Lemmatization
library(textstem)
getTransformations()
as.character(docs7[[1]])
docs8<-lemmatize_strings(docs7)
b<-unlist(str_split(docs8,"[,]"))
b
docs9
docs8 = lemmatize_strings(docs7)
docs9 = unlist(str_split(docs8,"[,]"))
docs9
dtm(docs9)
dtm<-DocumentTermMatrix(docs9)
dtm
dtm_m = as.matrix(dtm)
dtm_m
rowSums(dtm_m)
colSums(dtm_m)
freq = colSums(dtm_m)
freq
names(freq)
data.frame(term=names(freq), count=freq)
freqtable = data.frame(term=names(freq), count=freq)
freqtable
library(dpylr)
library(dplyr)
freqtable5 %>% filter(freq>=5)
freqtable5 = freqtable %>% filter(freq>=5)
freqtable5
# 5d) Word cloud
library(wordcloud)
wordcloud(names(freq),freq) #in general
wordcloud(names(freq),freq,colors=brewer.pal(8,"Darker"))
wordcloud(names(freq),freq,colors=brewer.pal(8,"Dark2"))
?brewer.pal
?wordcloud
wordcloud(names(freq),freq,colors=brewer.pal(8,"Dark2"), random.order = F)
ggplot(freqtable5,aes(x=term,y=count))+geom_bar(stat="identity")+
theme(axis.text.x=element_text(angle=45,hjust=1)) #Show all, include terms that hv small freq
freqtable5 %>% arrange(asc(count))
freqtable5 %>% arrange(ASC(count))
freqtable5 %>% arrange(DESC(count))
freqtable5 %>% arrange(desc(count))
ordered_freq_table = freqtable5 %>% arrange(desc(count))
ggplot(ordered_freq_table,aes(x=term,y=count))+geom_bar(stat="identity")+
theme(axis.text.x=element_text(angle=45,hjust=1)) #Show all, include terms that hv small freq
ggplot(ordered_freq_table,aes(x=term,y=count))+geom_bar(stat="identity")+
theme(axis.text.x=element_text(angle=45,hjust=1)) #Show all, include terms that hv small freq
ggplot(wf,aes(x=TERM,y=FREQ))+geom_bar(stat="identity")+
theme(axis.text.x=element_text(angle=45,hjust=1)) #Show all, include terms that hv small freq
ggplot(Subs,aes(x=TERM,y=FREQ))+geom_bar(stat="identity")+
theme(axis.text.x=element_text(angle=45,hjust=1))
ggplot(freqtable5,aes(x=term,y=count))+geom_bar(stat="identity")+
theme(axis.text.x=element_text(angle=45,hjust=1)) #Show all, include terms that hv small freq
ggplot(freqtable5 %>% filter(count>10),aes(x=term,y=count))+geom_bar(stat="identity")+
theme(axis.text.x=element_text(angle=45,hjust=1)) #Show all, include terms that hv small freq
# Barchart with freq > 5, too many terms
ggplot(freqtable5,aes(x=term,y=count))+geom_bar(stat="identity")+
theme(axis.text.x=element_text(angle=45,hjust=1)) #Show all, include terms that hv small freq
# Barchart with freq > 10
ggplot(freqtable5 %>% filter(count>10),aes(x=term,y=count))+geom_bar(stat="identity")+
theme(axis.text.x=element_text(angle=45,hjust=1)) #Show all, include terms that hv small freq
wordcloud(names(freq),freq,colors=brewer.pal(8,"Dark2"), random.order = F)
# 5e) Associated terms with "coast"
findAssocs(dtm, "coast", 0.1)
# 5e) Associated terms with "coast"
findAssocs(dtm, "coast", 0.2)
# 5e) Associated terms with "coast"
findAssocs(dtm, "coast", 0.3)
# Barchart with freq > 10
ggplot(freqtable5 %>% filter(count>10),aes(x=term,y=count))+geom_bar(stat="identity")+
theme(axis.text.x=element_text(angle=45,hjust=1)) #Show all, include terms that hv small freq
wordcloud(names(freq),freq,colors=brewer.pal(8,"Dark2"), random.order = F)
# 5e) Associated terms with "coast"
findAssocs(dtm, "coast", 0.2)
# R submission for test 1
# 4.
library(stringr)
monkey = read.csv("C:/Users/PC 12/Downloads/LittleMonkeys.csv",header=F)
monkey
# 4a)
str_view(monkey,"\\!")
# 4a)
str_view(monkey,"\\b.*\\!")
# 4a)
str_view(monkey,"\\b.?\\!")
# 4a)
str_view(monkey,"\\b.\\!")
# 4a)
str_view(monkey,"\\b...\\!")
?str_view
# 4a)
str_view(monkey,"\\b.*?\\!")
# 4a)
str_view(monkey,"\\b.*+\\!")
# 4a)
str_view(monkey,"\\b.*\\!")
# 4a)
str_view(monkey,"\\b.*?\\!")
# 4a)
str_view(monkey,"\\!")
