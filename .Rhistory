ww = c("statistics", "estate", "castrate", "catalyst", "Statistics")
print(grep(pattern="stat", x=ww)) # return index that contains "stat"
grep(pattern="stat", x=ww, ignore.case=T) # ignore case when searching for "stat"
grep(pattern="stat", x=ww, ignore.case=T, value=T) # return word instead of index
print('--------------------')
print(grepl(pattern="stat", x=ww)) # return boolen
print('--------------------')
print(regexpr(pattern="stat", ww))
gregexpr(pattern="stat",ww)
regexec(pattern="(st)(at)",ww)
print(regexpr(pattern="stat", ww))
sub('stat', 'STAT', ww, ignore.case=T)
gsub('stat', 'jai', ww, ignore.case=T)
sub('stat', 'STAT', ww, ignore.case=T)
gsub('stat', 'jai', ww, ignore.case=T)
sub('stat', 'STAT', ww, ignore.case=T)
str_length(ww)
library(stringr)
str_length(ww)
ww = c("statistics", "estate", "castrate", "catalyst", "Statistics", 'hazim fitri')
str_length(ww)
str_split(ww, " ")
str_c('a', 'b', 'c')
str_c('a', 'b', 'c', sep=',')
str_c('a,b', 'b,c', 'c,d', collapse=',') #
str_c("one for all","All for one",collapse=",") #combine the string to be one sentences
str_c("one for all","All for one")
str_sub(c(1,2,3), 1, 3)
str_sub(c(1,2,3), 1, 2)
#str_sub() gives subset
str_sub(x,1,3) #Gives from 1st to 3rd letter
x<-c("Apple","Banana","Pear")
#str_sub() gives subset
str_sub(x,1,3) #Gives from 1st to 3rd letter
str_sub(x,-3,-1) #Gives the last three letter
str_to_upper(ww)
str_to_lower(ww)
str_to_title(ww)
str_view(fruit, '.a.')
ex<-"aabbbccddddeeeee"
ex
str_view(ex, 'aab?')
str_view(ex, 'ccd?')
str_view(ex, 'dd?')
str_view(ex, 'aac?')
str_detect(fruit, "[aeiou]$")
fruit
str_count(fuit, 'e')
str_count(fruit, 'e')
str_replace(fruit, '^a', 'A')
str_replace(fruit, c('^a'='A', '^e'=E))
str_replace(fruit, c('^a'='A', '^e'=E))
str_replace(fruit, c('^a'='A', '^e'='E'))
str_replace_all(fruit, c('^a'='A', '^e'='E'))
docs<-Corpus(VectorSource(sentences))
library(tm)
library(tm)
docs<-Corpus(VectorSource(sentences))
writeLines(as.character(docs[[30]]))
getTransformations()
# dataset words, fruit, sentences
words
# dataset words, fruit, sentences
words
h = 'hai-hazim'
toSpace<-content_transformer(function(x,pattern){return(gsub(pattern," ",x))})
docs<-tm_map(docs,toSpace,"-")
h
h = 'hai-hazim'
toSpace<-content_transformer(function(x,pattern){return(gsub(pattern," ",x))})
tm_map(h,toSpace,"-")
h = 'hai-hazim'
h = Corpus(VectorSource(h))
toSpace<-content_transformer(function(x,pattern){return(gsub(pattern," ",x))})
tm_map(h,toSpace,"-")
inspect(h)
h = 'hello-world'
h = Corpus(VectorSource(h))
toSpace<-content_transformer(function(x,pattern){return(gsub(pattern," ",x))})
tm_map(h,toSpace,"-")
inspect(tm_map(h,toSpace,"-")
inspect(tm_map(h,toSpace,"-"))
inspect(tm_map(h,toSpace,"-"))
h = 'hello-world'
h = Corpus(VectorSource(h))
toSpace<-content_transformer(function(x,pattern){return(gsub(pattern," ",x))})
inspect(tm_map(h,toSpace,"-"))
library(tm)
docs = Corpus(VectorSource(sentences))
toSpace = content_transformer(function(x, pattern)
{return(gsub(pattern, " ", x))})
docs = tm_map(docs, toSpace, '-')
docs = tm_map(docs, removePunctuation)
docs = tm_map(docs, content_transformer(tolower))
docs = tm_map(docs, removeNumbers)
docs = tm_map(docs, removeWords, stopwords("english"))
docs = tm_map(docs, stripWhitespace)
docs = tm_map(docs, stemDocument) # stemming doc (e.g., eating -> eat)
dtm = TermDocumentMatrix(docs) # Term Document matrix
m = as.matrix(dtm)
v = sort(rowSums(m), decreasing=T)
df = data.frame(word=names(v), freq=v)
library(wordcloud2)
wordcloud2(df)
library(ggplot2)
subs = subset(df, freq > 10)
ggplot(subs)
ggplot(subs, aes(x=word, y=freq))
ggplot(subs, aes(x=word, y=freq)) +
geom_bar()
ggplot(subs, aes(x=word, y=freq)) +
geom_bar(stat='identity')
ggplot(subs, aes(x=word, y=freq)) +
geom_bar(stat='identity') +
theme(axis.text.x=element_text(angle=45))
ggplot(subs, aes(x=word, y=freq)) +
geom_bar(stat='identity') +
theme(axis.text.x=element_text(angle=45, hjust=1))
doc1 = readLines('data/Doc1.txt')
doc1 = readLines('02. Text Data Mining/movies/Doc1.txt')
doc1 = readLines('02. Text Data Mining/movies/Doc1.txt')
doc2 = readLines('02. Text Data Mining/movies/Doc2.txt')
doc3 = readLines('02. Text Data Mining/movies/Doc3.txt')
class(doc1)
class(doc2, doc3)
class(c(doc2, doc3))
class(doc2)
doc2
doc1
doc3
doc1 = readLines('02. Text Data Mining/movies/Doc1.txt')
doc2 = readLines('02. Text Data Mining/movies/Doc2.txt')
doc3 = readLines('02. Text Data Mining/movies/Doc3.txt')
doc4 = readLines('02. Text Data Mining/movies/Doc4.txt')
doc5 = readLines('02. Text Data Mining/movies/Doc5.txt')
doc4
doc5
gc = readLines('02. Text Data Mining/Attachment 1/GC.txt')
doc6 = read.csv('02. Text Data Mining/Attachment 1/Doc6.csv')
doc6
class(doc6)
doc6 = readLines('02. Text Data Mining/Attachment 1/Doc6.csv')
doc6
class(doc6)
# a warning will appear if the text does not have a final newline
doc1 = readLines('02. Text Data Mining/movies/Doc1.txt')
doc2 = readLines('02. Text Data Mining/movies/Doc2.txt')
doc3 = readLines('02. Text Data Mining/movies/Doc3.txt')
doc4 = readLines('02. Text Data Mining/movies/Doc4.txt')
doc5 = readLines('02. Text Data Mining/movies/Doc5.txt')
doc6 = readLines('02. Text Data Mining/Attachment 1/Doc6.csv')
gc = readLines('02. Text Data Mining/Attachment 1/GC.txt')
gccsv = readLines('02. Text Data Mining/Attachment 1/GC.csv')
rise = readLines('02. Text Data Mining/Attachment 1/Rise.csv')
gccsv
rise
eg1 <- read.table("02. Text Data Mining/Attachment 1/GC.txt", fill=T,header=F)
eg1
eg3 <- c("Hi!","Welcome to STQD6114","Tuesday, 11-1pm")
eg3
class(eg3)
doc5
class(doc5)
t = VectorSource(eg3)
t
inspect(t)
doc6
inspect(VCorpus(DataframeSource(doc6)))
doc1
doc2
doc3
doc4
doc5
docs =
Corpus(VectorSource(doc5))
docs
inpect(docs)
inspect(docs)
docs = Corpus(VectorSource(sentences))
inpsect(docs)
inspect(docs)
inspect(Corpus(VectorSource(doc6)))
inspect(Corpus(DataframeSource(doc6)))
inspect(Corpus(VectorSource(gccsv)))
