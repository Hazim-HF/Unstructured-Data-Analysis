---
title: "Unstructured Data Analytics"
author: "Hazim Fitri"
date: "`r Sys.Date()`"
output: pdf_document
always_allow_html: true
---

## Read data

```{r}
library(tm)
eg3 = c('Hi!', 'Welcome to STQD6114', )
```

# Text Data Mining

## Web Scrapping

```{r}
library(httr)
library(rvest)
library(dplyr)
library(XML)
```

```{r}
url = "https://www.amazon.com/s?k=laptop&language=en_US&crid=15XP9X39QRUQ6&currency=MYR&sprefix=lap%252Caps%252C468&ref=nb_sb_noss_2"

html_nodes(url, .a-price-whole)
```

# Text Exploration

```{r}
ww = c("statistics", "estate", "castrate", "catalyst", "Statistics", 'hazim fitri')
print(grep(pattern="stat", x=ww)) # return index that contains "stat"
grep(pattern="stat", x=ww, ignore.case=T) # ignore case when searching for "stat"
grep(pattern="stat", x=ww, ignore.case=T, value=T) # return word instead of index

print('--------------------')

print(grepl(pattern="stat", x=ww)) # return boolen

print('--------------------')

print(regexpr(pattern="stat", ww))
gregexpr(pattern="stat",ww)
regexec(pattern="(st)(at)",ww)

print('--------------------')

sub('stat', 'STAT', ww, ignore.case=T) # convert first match only
gsub('stat', 'jai', ww, ignore.case=T)

print('--------------------')

library(stringr)
str_length(ww)
str_split(ww, " ") # split a sentences by space and return a list
str_c('a', 'b', 'c') # combine multiple string
str_c('a', 'b', 'c', sep=',') # combine multiple string using seperator
str_c('a,b', 'b,c', 'c,d', collapse=',') # 
str_sub(c(1,2,3), 1, 2) # return the 1 ~ 2 character
str_to_upper(ww)
str_to_lower(ww)
str_to_title(ww)

str_view(fruit, '.a.') # return all 

str_detect

str_replace

print('--------------------')

ex<-"aabbbccddddeeeee"
str_view(ex, 'aac?')

str_detect
str_detect(fruit, "[aeiou]$") # a e i o u at the end
str_count(fruit, 'e') # count how many 'e' in each word
str_replace(fruit, '^a', 'A') # find word stat with 'a' and replace with 'A'
str_replace_all(fruit, c('^a'='A', '^e'='E'))
```

# Wordcloud

```{r}
library(tm)
docs<-Corpus(VectorSource(sentences))

writeLines(as.character(docs[[30]]))

getTransformations()
```

```{r}
library(stringr)
# dataset words, fruit, sentences
words
fruit
sentences
```

```{r}
h = 'hello-world'
h = Corpus(VectorSource(h))
toSpace<-content_transformer(function(x,pattern){return(gsub(pattern," ",x))})
inspect(tm_map(h,toSpace,"-"))

```

```{r wordcloud}
library(tm)
docs = Corpus(VectorSource(sentences))

toSpace = content_transformer(function(x, pattern)
  {return(gsub(pattern, " ", x))})

docs = tm_map(docs, toSpace, '-')

docs = tm_map(docs, removePunctuation)
docs = tm_map(docs, content_transformer(tolower))
docs = tm_map(docs, removeNumbers)
docs = tm_map(docs, removeWords, stopwords("english"))
docs = tm_map(docs, stripWhitespace)
docs = tm_map(docs, stemDocument) # stemming doc (e.g., eating -> eat)
dtm = TermDocumentMatrix(docs) # Term Document matrix
m = as.matrix(dtm)
v = sort(rowSums(m), decreasing=T)
df = data.frame(word=names(v), freq=v)

library(wordcloud2)
wordcloud2(df, shape = 'diamond')
```

```{r}
wordcloud2(df,
           shape    = "star",
           size     = 0.7,    # shrink if your big words are crowding out small ones
           minSize  = 0       # ensures even 1-freq terms get drawn
)
```

# Plot

```{r}
library(ggplot2)
subs = subset(df, freq > 10)
ggplot(subs, aes(x=word, y=freq)) +
  geom_bar(stat='identity') +
  theme(axis.text.x=element_text(angle=45, hjust=1))
```

## Exercise

```{r}
# a warning will appear if the text does not have a final newline
doc1 = readLines('02. Text Data Mining/movies/Doc1.txt')
doc2 = readLines('02. Text Data Mining/movies/Doc2.txt')
doc3 = readLines('02. Text Data Mining/movies/Doc3.txt')
doc4 = readLines('02. Text Data Mining/movies/Doc4.txt')
doc5 = readLines('02. Text Data Mining/movies/Doc5.txt')
doc6 = readLines('02. Text Data Mining/Attachment 1/Doc6.csv')
gc = readLines('02. Text Data Mining/Attachment 1/GC.txt')
gccsv = readLines('02. Text Data Mining/Attachment 1/GC.csv')
rise = readLines('02. Text Data Mining/Attachment 1/Rise.csv')



```

::: {style="font-family: 'Times New Roman'; font-size: 12pt; line-height: 1.5; font-weight: bold;"}
Hello World
:::

```{r}
eg6 = readLines("https://en.wikipedia.org/wiki/Data_science")
eg6[grep("\\h2",eg6)]
eg6[grep("\\p",eg6)] #paragraph


```

```{r}
library(XML)
doc = htmlParse(eg6)
doc.text = unlist(xpathApply(doc, '//p', xmlValue))
unlist(xpathApply(doc, '//h2', xmlValue))
```

```{r lect_example}
pages = paste0('https://www.amazon.co.jp/s?k=skincare&crid=28HIW1TYLV9UM&sprefix=skincare%2Caps%2C268&ref=nb_sb_noss_1&page=', 0:9)
eg10 = read_html (pages[1])
nodes = html_nodes (eg10,'.a-price-whole')
texts = html_text (nodes)

price = function(page){
  url = read_html(page)
  nodes = html_nodes(url, '.a-price-whole')
  html_text(nodes)
}

sapply(pages, price)
do.call("c", lapply(pages, price))
```

```{r project}
url = "https://www.lazada.com.my/shop-home-office-desks/?clickTrackInfo=matchType--20___description--57%2525%2Boff___seedItemMatchType--c2i___bucket--0___spm_id--category.hp___seedItemScore--0.0___abId--333258___score--0.095538795___pvid--ebb1cf54-2ab7-4654-827e-3ec1e410554b___refer--___appId--7253___seedItemId--4134886924___scm--1007.17253.333258.0___categoryId--10000768___timestamp--1748786136974&from=hp_categories&item_id=4134886924&page=4&params=%7B%22catIdLv1%22%3A%2210000340%22%2C%22pvid%22%3A%22ebb1cf54-2ab7-4654-827e-3ec1e410554b%22%2C%22src%22%3A%22ald%22%2C%22categoryName%22%3A%22Meja%2BRuang%2BKerja%22%2C%22categoryId%22%3A%2210000768%22%7D&q=meja%20ruang%20kerja&service=hp_categories&spm=a2o4k.homepage.categoriesPC.d_13_10000768&src=hp_categories&up_id=4134886924&version=v2"

page = read_html(url)

html_element(page, '.ooOxS') %>% html_text()

```

```{r F1_example}

library(rvest)

url = "https://en.wikipedia.org/wiki/List_of_Formula_One_drivers"

page = read_html(url)

html_element(page, "table.sortable")
```

```{r}
library(RSelenium)
library(rvest)

# Start Selenium server
rD <- rsDriver(browser = "chrome", port = 4445L, chromever = "latest")
remDr <- rD[["client"]]

# Navigate to page
remDr$navigate(url)

# Wait for page to load (important!)
Sys.sleep(5)

# Get page source and parse
page_source <- remDr$getPageSource()[[1]]
page <- read_html(page_source)

# Extract prices using more stable selectors
prices <- page %>%
  html_elements('[data-qa-locator="product-price"]') %>%
  html_text()

# Clean up
prices_clean <- gsub("RM|\\s", "", prices) %>% as.numeric()

# Close session
remDr$close()
rD[["server"]]$stop()

print(prices_clean)
```

```{r}
url = "https://www.imdb.com/search/title/?genres=action"

jai = read_html(url) %>% html_nodes("ipc-title")

jai
```

```{r}
library(rvest)
library(dplyr)

scrape_imdb_updated <- function(genre, page) {
  start <- (page - 1) * 50 + 1
  url <- paste0("https://www.imdb.com/search/title/?genres=", genre, "&title_type=feature&start=", start)
  page_content <- read_html(url)

  # New structure
  titles <- page_content %>% html_nodes("h3.ipc-title__text") %>% html_text(trim = TRUE)
  ratings <- page_content %>% html_nodes(".ratingGroup--imdb-rating") %>%
    html_text(trim = TRUE)
  
  # Optional: extract years, genres, etc., depending on structure.
  data.frame(
    Title = titles,
    Rating = ratings[1:length(titles)],  # Adjust if lengths mismatch
    stringsAsFactors = FALSE
  )
}

# Example usage
action_movies <- scrape_imdb_updated("action", 1)
head(action_movies)


```

```{r}
genres = c("action", "comedy")
```

```{r}
url = "https://www.imdb.com/search/title/?genres=action"

page = read_html(url)

page %>% html_nodes("h3.ipc-title__text") %>% html_text()
```

```{r}
page %>% html_nodes("span.ipc-rating-star--rating") %>% html_text
```

```{r}
page %>% html_nodes("span.ipc-rating-star--voteCount") %>% html_text()
```

```{r}
page %>% html_nodes("div.ipc-html-content-inner-div") %>% html_text
```

```{r}
page %>% html_nodes("a.ipc-link") %>% html_text
```

```{r}
page %>% html_nodes("span.sc-4b408797-8") %>% html_text
```

# Text Data Analysis

## Latent Dirichlet Allocation (LDA)

```{r}
library(topicmodels)

data("AssociatedPress")
AssociatedPress
```

```{r}
ap_lda = LDA(AssociatedPress, k=2, control=list(seed=1234))

ap_lda
```

```{r}
library(tidytext)
library(reshape2)

ap_topics = tidy(ap_lda, matrix='beta')
ap_topics
```

```{r}
library(ggplot2)
library(dplyr)

ap_top_terms = ap_topics %>% 
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>% 
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") + 
  coord_flip()
```

```{r}
library(tidyr)

beta_spread = ap_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > 0.005 | topic2 > 0.005) %>%
  mutate(log_ratio = log2(topic2 / topic1))
         
beta_spread
```

```{r}
beta_spread %>% 
  mutate(term=reorder(term,log_ratio))%>% 
  ggplot(aes(term,log_ratio)) +
  geom_col(show.legend=FALSE) +
  coord_flip()
```

```{r}
ap_documents = tidy(ap_lda, matrix = "gamma") # extract the per-document-per-topic-probabilities

ap_documents
```

```{r}
# check the most common words in the document eg document6
tidy(AssociatedPress) %>%
  filter(document==6) %>%
  arrange(desc(count))
```

## Text Cluster Analysis

### K-means

```{r}
mytext2 <- DirSource("04. Text Data Analysis/TextMining-20250517")
docsz <- Corpus(mytext2)
```

```{r}
getTransformations()
```

```{r}
docsz <- tm_map(docsz,content_transformer(tolower))

toSpace <- content_transformer(function(x,pattern){
  return(gsub(pattern," ", x))
})

docsz <- tm_map(docsz, toSpace, "-")
docsz <- tm_map(docsz, removePunctuation) #remove punctuation
docsz <- tm_map(docsz, removeNumbers) #Strip digits
docsz <- tm_map(docsz, removeWords, stopwords("english")) #remove stopwords
docsz <- tm_map(docsz, stripWhitespace) #remove whitespace
```

```{r}
dtm <-DocumentTermMatrix(docsz) #Create document term matrix
dtm
```

```{r}
## Present text data numerically, weighted TF-IDF
dtm.tfidf = weightTfIdf(dtm)
dtm.tfidf = removeSparseTerms(dtm.tfidf, 0.999)
tfidf.matrix = as.matrix(dtm.tfidf)
```

```{r}
## Cosine distance matrix (useful for specific clustering algorithms)
library(proxy)
dist.matrix = dist(tfidf.matrix , method = "cosine")
```

```{r}
## Perform clustering
truth.K=3

library(dbscan)
clustering.kmeans = kmeans(tfidf.matrix , truth.K)
clustering.hierarchical = hclust(dist.matrix , method = "ward.D2")
clustering.dbscan = hdbscan(dist.matrix , minPts=10)
```

```{r}
library(cluster)
clusplot(as.matrix(dist.matrix),
         clustering.kmeans$cluster,
         color=T,
         shade=T,
         labels=2,
         lines=0)
plot(clustering.hierarchical)
rect.hclust(clustering.hierarchical,3)
plot(as.matrix(dist.matrix),col=clustering.dbscan$cluster+1L)
```

```{r}
master.cluster = clustering.kmeans$cluster
slave.hierarchical = cutree(clustering.hierarchical, k = truth.K)
slave.dbscan = clustering.dbscan$cluster
```

```{r}
library(colorspace)
points = cmdscale(dist.matrix, k = 3)
palette = diverge_hcl(truth.K) # Creating a color palette
```

```{r}
plot(points, 
     main = 'K-Means Clustering', 
     col = as.factor(master.cluster),
     pch = 16,
     mai = c(0,0,0,0), 
     mar = c(0,0,0,0),
     xaxt = 'n', 
     yaxt = 'n', 
     xlab = '', 
     ylab ='')

plot(points, 
     main = 'Hierarchical Clustering', 
     col = as.factor(slave.hierarchical),
     pch = 16,
     mai = c(0,0,0,0), 
     mar = c(0,0,0,0),
     xaxt = 'n', 
     yaxt = 'n', 
     xlab = '', 
     ylab ='')

plot(points, 
     main = 'Densty-based Clustering', 
     pch = 16,
     col = as.factor(slave.dbscan),
     mai = c(0,0,0,0), 
     mar = c(0,0,0,0),
     xaxt = 'n', 
     yaxt = 'n', 
     xlab = '', 
     ylab ='')
```

```{r}
plot(points, 
     main = 'K-Means Clustering', 
     col = as.factor(master.cluster),
     pch = 19,
     mai = c(0,0,0,0), 
     mar = c(0,0,0,0),
     xaxt = 'n', 
     yaxt = 'n', 
     xlab = '', 
     ylab ='')

```

```{r}
table(master.cluster)
table(slave.hierarchical)
table(slave.dbscan)
```

```{r}
cost_df <- data.frame()
```

```{r}
for (i in 1:20) {
  kmeans <- kmeans(x=tfidf.matrix, centers=i, iter.max=100)
  cost_df <- rbind(cost_df, cbind(i,kmeans$tot.withinss))
}

names(cost_df) <- c("cluster","cost")
```

```{r}
plot(cost_df$cluster, cost_df$cost)
lines(cost_df$cluster, cost_df$cost)
```

## Sentiment Analysis

```{r data_prepping}
library(tm)
library(SnowballC)
library(wordcloud)
text=readLines(file.choose())
docs=Corpus(VectorSource(text))
inspect(docs)
toSpace=content_transformer(function(x,pattern)gsub(pattern," ",x))
docs=tm_map(docs, toSpace, "/")
docs=tm_map(docs, toSpace, "@")
docs=tm_map(docs, toSpace, "\\|")
docs=tm_map(docs,content_transformer(tolower))
docs=tm_map(docs,removeNumbers)
docs=tm_map(docs,removeWords, stopwords("english"))
docs=tm_map(docs,removeWords, c("dan","dengan","atau","sebagai","yang","itu",
                                "ini","asm","dari","daripada"))
docs=tm_map(docs,removePunctuation)
docs=tm_map(docs,stripWhitespace)
docs=tm_map(docs,stemDocument)
dtm=TermDocumentMatrix(docs)
m=as.matrix(dtm)
v=sort(rowSums(m),decreasing=TRUE)
d=data.frame(word=names(v),freq=v)
m=d$word

```

```{r sentiment_analysis}
mysentiment<-function(m)
{
  mydictpos=c("baik","cantik","bijak","kuat")
  mydictneg=c("jahat","buruk","bodoh","lemah")
  pos_score=sum(!is.na(match(m,mydictpos)))
  neg_score=(-1)*sum(!is.na(match(m,mydictneg)))
  sentiment_score=pos_score+neg_score
  sentiment_score
}
```

# Extra Example / Exercise

```{r}
knitr::opts_knit$set(root.dir = "D:/01. Education/02. Master/Semester 2/Unstructured-Data-Analysis/04. Text Data Analysis/TextMining")

getwd()
```

```{r}
setwd("D:/01. Education/02. Master/Semester 2/Unstructured-Data-Analysis/04. Text Data Analysis/TextMining")

getwd()
```

```{r}
getwd()
```

```{r}
# load text mining library
library(tm)
getwd()
```

```{r}
setwd("D:/01. Education/02. Master/Semester 2/Unstructured-Data-Analysis/04. Text Data Analysis/TextMining")

filenames = list.files(getwd(), pattern = "*.txt")
files = lapply(filenames, readLines, encoding = "latin1", warn = FALSE)
list.files()
```

```{r}
docs = VCorpus(VectorSource(files))
inspect(docs[[1]])
```

```{r}
toSpace <- content_transformer(function(x, pattern) { 
  return (gsub(pattern, " " , x))
  })

toSpaceUnicode <- content_transformer(function(x) gsub("[\u2012\u2013\u2014\u2015-]", " ", x))

docs_clean <- docs %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(toSpaceUnicode) %>%
  tm_map(removeNumbers) %>%
  tm_map(toSpace, "\\.") %>%
  tm_map(removePunctuation) %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(stripWhitespace)


inspect(docs_clean[[2]])
```

```{r}
writeLines(as.character(docs[[1]]))
```

## Sentiment

```{r}
library(tidytext)
library(textdata)
library(tidyverse)
library(syuzhet)
library(tm)
```

```{r}
get_sentiments("afinn")
```

```{r}
get_sentiments("bing")
```

```{r}
get_sentiments("nrc")
```

```{r}
text = readLines('04. Text Data Analysis/TeamHealthRawData.txt')

syuzhet_vector = get_sentiment(text, method = 'syuzhet')

bing_vector = get_sentiment(text, method = 'bing')

afinn = get_sentiment(text, method = 'afinn')
```

```{r}
library(tidyverse)

df <- data.frame(
  index = 1:length(syuzhet_vector),
  syuzhet = syuzhet_vector,
  bing = bing_vector,
  afinn = afinn_vector
)

df_long <- df %>%
  pivot_longer(cols = -index, names_to = "method", values_to = "sentiment")

ggplot(df_long, aes(x = index, y = sentiment, color = method)) +
  geom_line() +
  labs(title = "Sentiment Score Comparison", x = "Text Index", y = "Sentiment Score")

```

```{r}
ggplot(df_long, aes(x = method, y = sentiment, fill = method)) +
  geom_boxplot() +
  labs(title = "Distribution of Sentiment Scores")
```

```{r}
df_summary <- df_long %>%
  group_by(method) %>%
  summarise(mean_sentiment = mean(sentiment))

ggplot(df_summary, aes(x = method, y = mean_sentiment, fill = method)) +
  geom_col() +
  labs(title = "Average Sentiment Score by Method")

```

```{r}
df_polarity <- data.frame(
  method = rep(c("syuzhet", "bing", "afinn"), each = length(text)),
  sentiment = c(syuzhet_vector, bing_vector, afinn_vector)
)

df_polarity$polarity <- ifelse(df_polarity$sentiment > 0, "Positive",
                         ifelse(df_polarity$sentiment < 0, "Negative", "Neutral"))

library(dplyr)

df_summary <- df_polarity %>%
  group_by(method, polarity) %>%
  summarise(count = n(), .groups = "drop")

library(ggplot2)

ggplot(df_summary, aes(x = method, y = count, fill = polarity)) +
  geom_col(position = "dodge") +
  labs(title = "Sentiment Polarity Comparison by Method",
       x = "Method", y = "Count of Texts") +
  scale_fill_manual(values = c("Positive" = "green", "Negative" = "red", "Neutral" = "gray"))

```

```{r emotion}
nrc_vector = get_nrc_sentiment(text)
```
