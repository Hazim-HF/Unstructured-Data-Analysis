---
title: "TDA"
author: "Fitri"
date: "2025-07-15"
output: 
  pdf_document: 
    toc: true
    toc_depth: 5
---

# Latent Dirichlet Allocation (LDA)

```{r library}
library(topicmodels)
library(tidytext)
library(tidyverse)
```

```{r read_data}
library(topicmodels)
data("AssociatedPress")
AssociatedPress
```

```{r define_lda}
lda = LDA(AssociatedPress, k=2, control = list(seed=1234))
# k = 2 means that we want to categorize our dataset into two topics
lda
```

```{r}
library(tidytext)
topics = tidy(lda, matrix='beta')
topics
```

The output you're showing is the result of using **Latent Dirichlet Allocation (LDA)** on the `AssociatedPress` dataset, with `k = 2` topics. Here's a breakdown of what the tibble represents:

### **Output Explanation**

-   `topic` : The topic number assigned by LDA (either 1 or 2 in this case)

-   `term` : The word (token) from the vocabulary

-   `beta` : The probability of the term being generated from the topic (P(Terms))

### **How to Interpret It**

Each row corresponds to a term-topic pair and the estimated probability (`beta`) that the term belongs to that topic. For example:

-   The word `"aaron"` appears in **both topics**, but with different probabilities (`1.686917e-12` in topic 1 vs `3.895941e-05` in topic 2). It is more associated with topic 2.

-   `"abandon"` and its variants are distributed between both topics as well. `"abandoned"` has a higher beta value in topic 1 compared to topic 2, so it's more representative of topic 1.

### **Why Multiple Rows per Word?**

Because `tidy(lda, matrix = "beta")` returns the distribution of every word **across all topics**, so each term can appear up to `k` times (once per topic).

```{r}
library(tidyverse)
top_term = topics %>% group_by(topic) %>% top_n(10, beta) %>% arrange(topic, -beta)

top_term %>% mutate(term)
```

# Text Cluster Analysis

# Sentiment Analysis
