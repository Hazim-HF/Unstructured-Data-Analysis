{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6da0d74",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwordcloud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "######################## Text Exploration ########################\n",
    "\n",
    "# Regular expression - a language to identify pattern/sequence of character\n",
    "\n",
    "## grep functions (without using any package)\n",
    "ww = [\"statistics\", \"estate\", \"castrate\", \"catalyst\", \"Statistics\"]\n",
    "ss = [\"I like statistics\", \"I like bananas\", \"Estates and statues are expensive\"]\n",
    "\n",
    "# 1st function - grep equivalent - give the location/index of pattern\n",
    "def grep(pattern, strings, ignore_case=False, value=False):\n",
    "    flags = re.IGNORECASE if ignore_case else 0\n",
    "    matches = [(i, s) for i, s in enumerate(strings) if re.search(pattern, s, flags)]\n",
    "    if value:\n",
    "        return [s for i, s in matches]\n",
    "    return [i for i, s in matches]\n",
    "\n",
    "print(grep(\"stat\", ww))  # returns indices\n",
    "print(grep(\"stat\", ww, ignore_case=True))  # ignore case, returns indices\n",
    "print(grep(\"stat\", ww, ignore_case=True, value=True))  # ignore case, returns matched strings\n",
    "\n",
    "# 2nd function - grepl equivalent - give logical expression\n",
    "def grepl(pattern, strings, ignore_case=False):\n",
    "    flags = re.IGNORECASE if ignore_case else 0\n",
    "    return [bool(re.search(pattern, s, flags)) for s in strings]\n",
    "\n",
    "print(grepl(\"stat\", ww))  # Return true/false\n",
    "print(grepl(\"stat\", ss))\n",
    "\n",
    "# 3rd function - regexpr equivalent\n",
    "def regexpr(pattern, strings):\n",
    "    results = []\n",
    "    for s in strings:\n",
    "        match = re.search(pattern, s)\n",
    "        if match:\n",
    "            results.append((match.start(), match.end() - match.start()))\n",
    "        else:\n",
    "            results.append((-1, -1))\n",
    "    return results\n",
    "\n",
    "print(regexpr(\"stat\", ww))\n",
    "print(regexpr(\"stat\", ss))\n",
    "\n",
    "# 4th function - gregexpr equivalent\n",
    "def gregexpr(pattern, strings):\n",
    "    return [[(m.start(), m.end() - m.start()) for m in re.finditer(pattern, s)] for s in strings]\n",
    "\n",
    "print(gregexpr(\"stat\", ss))\n",
    "\n",
    "# 5th function - regexec equivalent\n",
    "def regexec(pattern, strings):\n",
    "    results = []\n",
    "    for s in strings:\n",
    "        match = re.search(pattern, s)\n",
    "        if match:\n",
    "            groups = match.groups()\n",
    "            results.append((match.start(), match.end() - match.start(), groups) if groups else (match.start(), match.end() - match.start()))\n",
    "        else:\n",
    "            results.append((-1, -1))\n",
    "    return results\n",
    "\n",
    "print(regexec(\"(st)(at)\", ww))\n",
    "\n",
    "# 6th function - sub equivalent\n",
    "def sub(pattern, replacement, strings, ignore_case=False):\n",
    "    flags = re.IGNORECASE if ignore_case else 0\n",
    "    return [re.sub(pattern, replacement, s, flags=flags) for s in strings]\n",
    "\n",
    "print(sub(\"stat\", \"STAT\", ww, ignore_case=True))\n",
    "print(sub(\"stat\", \"STAT\", ss, ignore_case=True))\n",
    "\n",
    "# 7th function - gsub equivalent (same as sub in Python)\n",
    "def gsub(pattern, replacement, strings, ignore_case=False):\n",
    "    return sub(pattern, replacement, strings, ignore_case)\n",
    "\n",
    "print(gsub(\"stat\", \"STAT\", ss, ignore_case=True))\n",
    "\n",
    "# Common string operations\n",
    "text = \"This is STQD6114\"\n",
    "print(len(text))  # str_length equivalent\n",
    "\n",
    "sentences = [\"This is a sentence.\", \"Another sentence here.\"]\n",
    "print([s.split() for s in sentences])  # str_split equivalent\n",
    "\n",
    "print(\"\".join([\"a\", \"b\", \"c\"]))  # str_c equivalent\n",
    "print([\"A\" + s for s in [\"li\", \"bu\", \"ngry\"]])  # str_c with vector\n",
    "print(\", \".join([\"one for all\", \"All for one\"]))  # str_c with sep\n",
    "\n",
    "x = [\"Apple\", \"Banana\", \"Pear\"]\n",
    "\n",
    "# str_sub equivalent\n",
    "print([s[:3] for s in x])  # First 3 characters\n",
    "print([s[-3:] for s in x])  # Last 3 characters\n",
    "\n",
    "print([s.upper() for s in x])  # str_to_upper\n",
    "print([s.lower() for s in x])  # str_to_lower\n",
    "print(\"Unstructured Data Analytics\".title())  # str_to_title\n",
    "\n",
    "# Regular expression patterns\n",
    "fruit = [\"apple\", \"banana\", \"pear\", \"orange\", \"grape\"]\n",
    "print([f for f in fruit if re.search(\"an\", f)])  # str_view equivalent\n",
    "print([f for f in fruit if re.search(\"^a\", f)])  # starts with a\n",
    "print([f for f in fruit if re.search(\"a$\", f)])  # ends with a\n",
    "print([f for f in fruit if re.search(\"^...$\", f)])  # exactly 3 characters\n",
    "\n",
    "ee = [\"sum\", \"summarize\", \"rowsum\", \"summary\"]\n",
    "print([s for s in ee if re.search(r\"\\bsum\", s)])  # starts with sum\n",
    "print([s for s in ee if re.search(r\"sum\\b\", s)])  # ends with sum\n",
    "\n",
    "ss_num = [\"This is a class with students\", \"There are 18 students\", \"This class is from 11.00 am\"]\n",
    "print([s for s in ss_num if re.search(r\"\\d\", s)])  # contains digits\n",
    "print([s for s in ss_num if re.search(r\"\\s\", s)])  # contains whitespace\n",
    "\n",
    "# Character classes\n",
    "print([f for f in fruit if re.search(r\"[abc]\", f)])  # contains a, b, or c\n",
    "print([f for f in fruit if re.search(r\"^[abc]\", f)])  # starts with a, b, or c\n",
    "\n",
    "# Repetition\n",
    "ex = \"aabbbccddddeeeee\"\n",
    "print(re.findall(r\"aab?\", ex))\n",
    "print(re.findall(r\"(a|d){2}\", ex))\n",
    "print(re.findall(r\"de+\", ex))\n",
    "print(re.findall(r\"\\d+\", \"There are 123 apples and 45 bananas\"))\n",
    "\n",
    "# Grouping and backreferencing\n",
    "print([f for f in fruit if re.search(r\"(a).\\1\", f)])  # a followed by any char, then a\n",
    "print([f for f in fruit if re.search(r\"(a)(.)\\1\\2\", f)])  # a, any char, a, same char\n",
    "\n",
    "# Text cleaning and preprocessing\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text)\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Example cleaning\n",
    "sample_text = \"This is a sample text with some numbers 123 and punctuation!\"\n",
    "print(clean_text(sample_text))\n",
    "\n",
    "# Word frequency analysis\n",
    "corpus = [\"This is the first document.\",\n",
    "          \"This document is the second document.\",\n",
    "          \"And this is the third one.\",\n",
    "          \"Is this the first document?\"]\n",
    "\n",
    "# Create word frequency\n",
    "words = []\n",
    "for doc in corpus:\n",
    "    words.extend(word_tokenize(clean_text(doc)))\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# Create DataFrame\n",
    "wf = pd.DataFrame.from_dict(word_freq, orient='index', columns=['FREQ'])\n",
    "wf.index.name = 'TERM'\n",
    "wf = wf.reset_index()\n",
    "print(wf.head())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "wf[wf['FREQ'] >= 2].plot.bar(x='TERM', y='FREQ')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Wordcloud\n",
    "wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(word_freq)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# For more advanced wordclouds (like wordcloud2 in R), you might need to use libraries like:\n",
    "# from wordcloud import WordCloud, ImageColorGenerator\n",
    "# or other visualization libraries like plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb27599",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstem\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer, WordNetLemmatizer\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Text processing functions\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text)\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Process and count words\n",
    "words = []\n",
    "for doc in corpus:\n",
    "    words.extend(word_tokenize(clean_text(doc)))\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# Create DataFrame\n",
    "wf = pd.DataFrame.from_dict(word_freq, orient='index', columns=['FREQ'])\n",
    "wf.index.name = 'TERM'\n",
    "wf = wf.reset_index()\n",
    "\n",
    "# Plot word frequencies\n",
    "plt.figure(figsize=(10, 5))\n",
    "wf[wf['FREQ'] >= 2].plot.bar(x='TERM', y='FREQ')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Word Cloud with error handling\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "    \n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"WordCloud package not installed. Please install it using:\")\n",
    "    print(\"pip install wordcloud\")\n",
    "    print(\"Here's a bar chart instead:\")\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    wf.sort_values('FREQ', ascending=False).head(10).plot.bar(x='TERM', y='FREQ')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "157bc46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "movie = pd.read_table('movies/Doc1.txt')\n",
    "movie.head()\n",
    "print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ad01b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\user\\\\Documents\\\\Unstructured-Data-Analysis\\\\02. Text Data Mining'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
